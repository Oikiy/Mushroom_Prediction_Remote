{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom Prediction: A Preliminary Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a Juptyer notebook for the Kaggle Project: Mushroom Classification\n",
    "# %pip install ydata-profiling\n",
    "# %pip install numpy\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install --upgrade matplotlib\n",
    "# %pip install --upgrade seaborn\n",
    "# %pip install --upgrade scikit-learn\n",
    "# %pip install --upgrade scipy\n",
    "# %pip install --upgrade catboost\n",
    "# %pip install --upgrade xgboost\n",
    "# %pip install --upgrade lightgbm\n",
    "# %pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libaries\n",
    "import os\n",
    "\n",
    "## Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "## Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import set_config\n",
    "from ydata_profiling import ProfileReport\n",
    "%matplotlib inline \n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "# Machine learning_ Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# # Model selection\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "#Palette\n",
    "palette = ['#328ca9', '#0e6ea9', '#2c4ea3', '#193882', '#102446']\n",
    "\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set the configuration of sklearn\n",
    "SEED = 42 # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Preprocessing\n",
    "\n",
    "Next, we preprocess the data by further imputing the missing values, one hot encoding the options in every features and label encoding the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "\n",
    "\n",
    "# Specify the data types for columns with mixed types\n",
    "dtype_spec = {\n",
    "    'cap-diameter': 'float16',\n",
    "    'stem-height': 'float16',\n",
    "    'stem-width': 'float16',\n",
    "    'does-bruise-or-bleed':'category',\n",
    "    'has-ring':'category'\n",
    "}\n",
    "\n",
    "train_df = pd.read_csv(r'Output\\\\Cleaned_Data\\\\train_cleaned.csv',dtype=dtype_spec)\n",
    "test_df = pd.read_csv(r'Output\\\\Cleaned_Data\\\\test_cleaned.csv',dtype=dtype_spec)\n",
    "y = pd.read_csv(r'Output\\\\Cleaned_Data\\\\target.csv',dtype='category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kai Qi Yan\\anaconda3\\envs\\MLEnv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# visualizing pipeline\n",
    "set_config(display='diagram')\n",
    "\n",
    "from utils import PreprocessData\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preprocess the data\n",
    "X, preprocessor = PreprocessData(train_df)\n",
    "X1, preprocessor = PreprocessData(test_df)\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y).ravel()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 1st Level models\n",
    "\n",
    "Then, we proceed to construct the 1st level models, which begins by defining the models (#6.1) and their parameters (#6.2). In this project, we will tune the hyperparameters by RandomizedSearchCV, and thus a parameter grid is defined in Section 6.2.\n",
    "\n",
    "### 6.1. Model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sort_dict\n",
    "\n",
    "# Define a list of models for prediction\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=SEED),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(random_state=SEED),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(random_state=SEED),\n",
    "    \"XGBClassifier\": XGBClassifier(random_state=SEED),\n",
    "    \"MLP Classifier\": MLPClassifier(random_state=SEED),\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(random_state=SEED),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth=1), algorithm='SAMME',random_state=SEED),\n",
    "    \"Dummy Classifier\": DummyClassifier(strategy='most_frequent',random_state=SEED)  # DummyClassifier for sanity check\n",
    "}\n",
    "\n",
    "# Sort the models\n",
    "classifiers=sort_dict(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the hyperparamter tuning of models by RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "\n",
    "params_classifiers = {\n",
    "\n",
    "    \"Logistic Regression\": {\n",
    "        'solver': ['newton-cg', 'sag', 'lbfgs'],  \n",
    "        'penalty': ['l2'],  \n",
    "        'C': [0.1],\n",
    "        'max_iter': [100, 200, 300]\n",
    "    },\n",
    "\n",
    "    \"Random Forest Classifier\": {\n",
    "        'n_estimators': [64, 128, 256],\n",
    "        'max_depth': [8, 16, 32, 64],\n",
    "        'criterion': ['entropy'],\n",
    "        'warm_start': [False]\n",
    "    },\n",
    "\n",
    "    \"Gradient Boosting Classifier\": {\n",
    "        'learning_rate': stats.loguniform(1e-2, 1e-1),\n",
    "        'n_estimators': [8, 16, 32, 64, 128, 256],\n",
    "        'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9],\n",
    "    },\n",
    "\n",
    "\n",
    "    \"XGBClassifier\": {\n",
    "            'objective':['binary:logistic'],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "            'n_estimators': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "\n",
    "\n",
    "    \"MLP Classifier\": {\n",
    "        'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': stats.loguniform(1e-5, 1e-2),\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    },\n",
    "    \n",
    "    \"Extra Trees Classifier\": {\n",
    "        'n_estimators': [128, 256,524],\n",
    "        'criterion': ['entropy'],\n",
    "        'max_features': [10, 20, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False],\n",
    "        'warm_start': [False]\n",
    "    },\n",
    "\n",
    "    \"AdaBoost Classifier\": {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'learning_rate': stats.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "\n",
    "    \"Dummy Classifier\": {}\n",
    "}\n",
    "\n",
    "# Sort the parameters\n",
    "params_classifiers = sort_dict(params_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Model training and hyperparameters tuning\n",
    "\n",
    "#### a. Setup scoring method for the model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Setup the KFold\n",
    "NFOLDS = 3 # set folds for out-of-fold prediction\n",
    "kf = StratifiedKFold(n_splits= NFOLDS,shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.b. Hyperparameter tuning using a smaller set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #999999; padding: 10px; border-radius: 5px; background-color: #282828; max-width: 97.5%; overflow-x: auto;\">\n",
    "<p>\n",
    "<br>- Because we have a huge dataset, I decided to use a subset of which for tuning the hyperparameters and select models. The selected models will then be retrained in the whole dataset.\n",
    "<br>- We made a subset of sample size= 100000, and\n",
    "<br>- We selected models which show MCC score > 0.8 for further training.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Running model: AdaBoost Classifier\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Model-tuning success: AdaBoost Classifier Best Parameters: {'learning_rate': 0.006251373574521752, 'n_estimators': 200}\n",
      "Predicting\n",
      "Model-prediction success: AdaBoost Classifier MCC_train: 0.27887373091080625  , MCC_val: 0.26983651962177213\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: Dummy Classifier\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Model-tuning success: Dummy Classifier Best Parameters: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kai Qi Yan\\anaconda3\\envs\\MLEnv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=5. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting\n",
      "Model-prediction success: Dummy Classifier MCC_train: 0.0  , MCC_val: 0.0\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: Extra Trees Classifier\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Model-tuning success: Extra Trees Classifier Best Parameters: {'warm_start': False, 'n_estimators': 256, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 10, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Predicting\n",
      "Model-prediction success: Extra Trees Classifier MCC_train: 0.9845917266473769  , MCC_val: 0.9755824261402724\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: Gradient Boosting Classifier\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Model-tuning success: Gradient Boosting Classifier Best Parameters: {'learning_rate': 0.06021310185147604, 'n_estimators': 128, 'subsample': 0.7}\n",
      "Predicting\n",
      "Model-prediction success: Gradient Boosting Classifier MCC_train: 0.8426947952989458  , MCC_val: 0.8474524381488361\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: Logistic Regression\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Model-tuning success: Logistic Regression Best Parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 200, 'C': 0.1}\n",
      "Predicting\n",
      "Model-prediction success: Logistic Regression MCC_train: 0.4641859914531132  , MCC_val: 0.45796713737993994\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: MLP Classifier\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Model-tuning success: MLP Classifier Best Parameters: {'activation': 'tanh', 'alpha': 0.0024526126311336773, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "Predicting\n",
      "Model-prediction success: MLP Classifier MCC_train: 0.9882990456888804  , MCC_val: 0.975381076468687\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: Random Forest Classifier\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Model-tuning success: Random Forest Classifier Best Parameters: {'warm_start': False, 'n_estimators': 256, 'max_depth': 32, 'criterion': 'entropy'}\n",
      "Predicting\n",
      "Model-prediction success: Random Forest Classifier MCC_train: 1.0  , MCC_val: 0.9806277323032292\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: XGBClassifier\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Model-tuning success: XGBClassifier Best Parameters: {'objective': 'binary:logistic', 'n_estimators': 128, 'max_depth': 9, 'colsample_bytree': 0.6}\n",
      "Predicting\n",
      "Model-prediction success: XGBClassifier MCC_train: 0.9934174990008907  , MCC_val: 0.9789141279417274\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Model saved: XGBClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>MCC_Score_Train_sample</th>\n",
       "      <th>MCC_Score_val_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dummy Classifier</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost Classifier</td>\n",
       "      <td>0.278874</td>\n",
       "      <td>0.269837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.464186</td>\n",
       "      <td>0.457967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.842695</td>\n",
       "      <td>0.847452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP Classifier</td>\n",
       "      <td>0.988299</td>\n",
       "      <td>0.975381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.984592</td>\n",
       "      <td>0.975582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.993417</td>\n",
       "      <td>0.978914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model Name  MCC_Score_Train_sample  MCC_Score_val_sample\n",
       "1              Dummy Classifier                0.000000              0.000000\n",
       "0           AdaBoost Classifier                0.278874              0.269837\n",
       "4           Logistic Regression                0.464186              0.457967\n",
       "3  Gradient Boosting Classifier                0.842695              0.847452\n",
       "5                MLP Classifier                0.988299              0.975381\n",
       "2        Extra Trees Classifier                0.984592              0.975582\n",
       "7                 XGBClassifier                0.993417              0.978914\n",
       "6      Random Forest Classifier                1.000000              0.980628"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import model_evaluation\n",
    "\n",
    "# Sampling data for hyperparameter tuning\n",
    "sample_size = 100000  # sample size for tuning\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, shuffle=True, random_state=SEED, stratify=y)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_sample, X_val_sample, y_train_sample, y_val_sample = train_test_split(X_sample, y_sample, test_size=0.2, shuffle=True, random_state=SEED,stratify=y_sample)\n",
    "\n",
    "# MCC Scores\n",
    "model_list_tuning, MCC_train_list_tuning, MCC_val_list_tuning,y_train_pred_list_tuning, model_params_tuning = model_evaluation(classifiers, X_train_sample, y_train_sample, X_val_sample, y_val_sample, kf, params= params_classifiers, mode='tuning')\n",
    "\n",
    "# Display the scores\n",
    "pd.DataFrame(list(zip(model_list_tuning, MCC_train_list_tuning, MCC_val_list_tuning)), columns=['Model Name', 'MCC_Score_Train_sample', 'MCC_Score_val_sample']).sort_values(by=[\"MCC_Score_val_sample\"],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Classifier\n",
      "Gradient Boosting Classifier\n",
      "MLP Classifier\n",
      "Random Forest Classifier\n",
      "XGBClassifier\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Obtain a subset of models that has MCC_Score_val > 0.8\n",
    "models_selected = []\n",
    "parameters_selected =[]\n",
    "for i in range(len(list(classifiers))):\n",
    "    if MCC_val_list_tuning[i] > 0.8:\n",
    "        print(list(classifiers.keys())[i])\n",
    "        models_selected.append(list(classifiers.keys())[i])\n",
    "        parameters_selected.append(model_params_tuning[i])\n",
    "\n",
    "# Display the selected models\n",
    "models_selected\n",
    "\n",
    "# Select the models from the classifiers dictionary\n",
    "classifiers_selected = {key: copy.deepcopy(classifiers[key]) for key in models_selected}\n",
    "\n",
    "# add the parameters to the selected models\n",
    "params_classifiers_selected = {key: parameters_selected[i] for i,key in enumerate(models_selected)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extra Trees Classifier': ExtraTreesClassifier(random_state=42),\n",
       " 'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=42),\n",
       " 'MLP Classifier': MLPClassifier(random_state=42),\n",
       " 'Random Forest Classifier': RandomForestClassifier(random_state=42),\n",
       " 'XGBClassifier': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=42, ...)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers_selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extra Trees Classifier': {'warm_start': False,\n",
       "  'n_estimators': 256,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 2,\n",
       "  'max_features': 10,\n",
       "  'criterion': 'entropy',\n",
       "  'bootstrap': False},\n",
       " 'Gradient Boosting Classifier': {'learning_rate': 0.06021310185147604,\n",
       "  'n_estimators': 128,\n",
       "  'subsample': 0.7},\n",
       " 'MLP Classifier': {'activation': 'tanh',\n",
       "  'alpha': 0.0024526126311336773,\n",
       "  'hidden_layer_sizes': (100,),\n",
       "  'learning_rate': 'constant',\n",
       "  'solver': 'adam'},\n",
       " 'Random Forest Classifier': {'warm_start': False,\n",
       "  'n_estimators': 256,\n",
       "  'max_depth': 32,\n",
       "  'criterion': 'entropy'},\n",
       " 'XGBClassifier': {'objective': 'binary:logistic',\n",
       "  'n_estimators': 128,\n",
       "  'max_depth': 9,\n",
       "  'colsample_bytree': 0.6}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_classifiers_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Running model: Extra Trees Classifier\n",
      "training fold: # 1\n",
      "training fold: # 2\n",
      "training fold: # 3\n",
      "Model-training success: Extra Trees Classifier\n",
      "Model: Extra Trees Classifier oof predictions and val predictions saved successfully\n",
      "Predicting\n",
      "Model-prediction success: Extra Trees Classifier MCC_train: 0.9908697793725013  , MCC_val: 0.9736660791126864\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: Gradient Boosting Classifier\n",
      "training fold: # 1\n",
      "training fold: # 2\n",
      "training fold: # 3\n",
      "Model-training success: Gradient Boosting Classifier\n",
      "Model: Gradient Boosting Classifier oof predictions and val predictions saved successfully\n",
      "Predicting\n",
      "Model-prediction success: Gradient Boosting Classifier MCC_train: 0.8569308286661608  , MCC_val: 0.8624816628440494\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: MLP Classifier\n",
      "training fold: # 1\n",
      "training fold: # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kai Qi Yan\\anaconda3\\envs\\MLEnv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fold: # 3\n",
      "Model-training success: MLP Classifier\n",
      "Model: MLP Classifier oof predictions and val predictions saved successfully\n",
      "Predicting\n",
      "Model-prediction success: MLP Classifier MCC_train: 0.9821675281604441  , MCC_val: 0.9721505620936095\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: Random Forest Classifier\n",
      "training fold: # 1\n",
      "training fold: # 2\n",
      "training fold: # 3\n",
      "Model-training success: Random Forest Classifier\n",
      "Model: Random Forest Classifier oof predictions and val predictions saved successfully\n",
      "Predicting\n",
      "Model-prediction success: Random Forest Classifier MCC_train: 0.9931658002058593  , MCC_val: 0.9801253521320538\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Running model: XGBClassifier\n",
      "training fold: # 1\n",
      "training fold: # 2\n",
      "training fold: # 3\n",
      "Model-training success: XGBClassifier\n",
      "Model: XGBClassifier oof predictions and val predictions saved successfully\n",
      "Predicting\n",
      "Model-prediction success: XGBClassifier MCC_train: 0.9825517259791553  , MCC_val: 0.9759874531974972\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Model saved: XGBClassifier\n"
     ]
    }
   ],
   "source": [
    "# Split the entire data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the selected models with the entire training data\n",
    "model_list, MCC_train_list, MCC_val_list, y_train_pred_list, oof_predictions_df, val_predictions_df = model_evaluation(classifiers_selected , X_train_sample, y_train_sample, X_val_sample, y_val_sample, kf, params= params_classifiers_selected, mode='training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
