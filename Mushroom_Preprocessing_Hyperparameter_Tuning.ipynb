{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom Prediction: A Preliminary Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a Juptyer notebook for the Kaggle Project: Mushroom Classification\n",
    "# %pip install ydata-profiling\n",
    "# %pip install numpy\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install --upgrade matplotlib\n",
    "# %pip install --upgrade seaborn\n",
    "# %pip install --upgrade scikit-learn\n",
    "# %pip install --upgrade scipy\n",
    "# %pip install --upgrade catboost\n",
    "# %pip install --upgrade xgboost\n",
    "# %pip install --upgrade lightgbm\n",
    "# %pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libaries\n",
    "import os\n",
    "\n",
    "## Data analysis and wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "## Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import set_config\n",
    "from ydata_profiling import ProfileReport\n",
    "%matplotlib inline \n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "# Machine learning_ Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# # Model selection\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "#Palette\n",
    "palette = ['#328ca9', '#0e6ea9', '#2c4ea3', '#193882', '#102446']\n",
    "\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set the configuration of sklearn\n",
    "SEED = 42 # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Preprocessing\n",
    "\n",
    "Next, we preprocess the data by further imputing the missing values, one hot encoding the options in every features and label encoding the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "# Specify the data types for columns with mixed types\n",
    "dtype_spec = {\n",
    "    'cap-diameter': 'float16',\n",
    "    'stem-height': 'float16',\n",
    "    'stem-width': 'float16',\n",
    "    'does-bruise-or-bleed':'category',\n",
    "    'has-ring':'category'\n",
    "}\n",
    "\n",
    "train_df = pd.read_csv(r'Output\\train_cleaned.csv',dtype=dtype_spec)\n",
    "test_df = pd.read_csv(r'Output\\test_cleaned.csv',dtype=dtype_spec)\n",
    "y = pd.read_csv(r'Output\\target.csv',dtype='category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing pipeline\n",
    "set_config(display='diagram')\n",
    "\n",
    "from utils import PreprocessData\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preprocess the data\n",
    "X, preprocessor = PreprocessData(train_df)\n",
    "X1, preprocessor = PreprocessData(test_df)\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y).ravel()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 1st Level models\n",
    "\n",
    "Then, we proceed to construct the 1st level models, which begins by defining the models (#6.1) and their parameters (#6.2). In this project, we will tune the hyperparameters by RandomizedSearchCV, and thus a parameter grid is defined in Section 6.2.\n",
    "\n",
    "### 6.1. Model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sort_dict\n",
    "\n",
    "# Define a list of models for prediction\n",
    "classifiers = {\n",
    "    # \"Logistic Regression\": LogisticRegression(random_state=SEED),\n",
    "    # \"Random Forest Classifier\": RandomForestClassifier(random_state=SEED),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(random_state=SEED),\n",
    "    # \"XGBClassifier\": XGBClassifier(random_state=SEED),\n",
    "    # \"MLP Classifier\": MLPClassifier(random_state=SEED),\n",
    "    # \"Extra Trees Classifier\": ExtraTreesClassifier(random_state=SEED),\n",
    "    # \"AdaBoost Classifier\": AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth=1), algorithm='SAMME',random_state=SEED),\n",
    "    # \"Dummy Classifier\": DummyClassifier(strategy='most_frequent',random_state=SEED)  # DummyClassifier for sanity check\n",
    "}\n",
    "\n",
    "# Sort the models\n",
    "classifiers=sort_dict(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the hyperparamter tuning of models by RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "\n",
    "params_classifiers = {\n",
    "\n",
    "    \"Logistic Regression\": {\n",
    "        'solver': ['newton-cg', 'sag', 'lbfgs'],  \n",
    "        'penalty': ['l2'],  \n",
    "        'C': [0.1],\n",
    "        'max_iter': [100, 200, 300]\n",
    "    },\n",
    "\n",
    "    \"Random Forest Classifier\": {\n",
    "        'n_estimators': [64, 128, 256],\n",
    "        'max_depth': [8, 16, 32, 64],\n",
    "        'criterion': ['entropy'],\n",
    "        'warm_start': [False]\n",
    "    },\n",
    "\n",
    "    \"Gradient Boosting Classifier\": {\n",
    "        'learning_rate': stats.loguniform(1e-2, 1e-1),\n",
    "        'n_estimators': [8, 16, 32, 64, 128, 256],\n",
    "        'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9],\n",
    "    },\n",
    "\n",
    "\n",
    "    \"XGBClassifier\": {\n",
    "            'objective':['binary:logistic'],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "            'n_estimators': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "\n",
    "\n",
    "    \"MLP Classifier\": {\n",
    "        'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': stats.loguniform(1e-5, 1e-2),\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    },\n",
    "    \n",
    "    \"Extra Trees Classifier\": {\n",
    "        'n_estimators': [128, 256,524],\n",
    "        'criterion': ['entropy'],\n",
    "        'max_features': [10, 20, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False],\n",
    "        'warm_start': [False]\n",
    "    },\n",
    "\n",
    "    \"AdaBoost Classifier\": {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'learning_rate': stats.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "\n",
    "    \"Dummy Classifier\": {}\n",
    "}\n",
    "\n",
    "# Sort the parameters\n",
    "params_classifiers = sort_dict(params_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Model training and hyperparameters tuning\n",
    "\n",
    "#### a. Setup scoring method for the model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Setup the KFold\n",
    "NFOLDS = 3 # set folds for out-of-fold prediction\n",
    "kf = StratifiedKFold(n_splits= NFOLDS,shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.b. Hyperparameter tuning using a smaller set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #999999; padding: 10px; border-radius: 5px; background-color: #282828; max-width: 97.5%; overflow-x: auto;\">\n",
    "<p>\n",
    "<br>- Because we have a huge dataset, I decided to use a subset of which for tuning the hyperparameters and select models. The selected models will then be retrained in the whole dataset.\n",
    "<br>- We made a subset of sample size= 100000, and\n",
    "<br>- We selected models which show MCC score > 0.8 for further training.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import model_evaluation\n",
    "\n",
    "# Sampling data for hyperparameter tuning\n",
    "sample_size = 100000  # sample size for tuning\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, shuffle=True, random_state=SEED, stratify=y)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_sample, X_val_sample, y_train_sample, y_val_sample = train_test_split(X_sample, y_sample, test_size=0.2, shuffle=True, random_state=SEED,stratify=y_sample)\n",
    "\n",
    "# MCC Scores\n",
    "model_list_tuning, MCC_train_list_tuning, MCC_val_list_tuning,y_train_pred_list_tuning, model_params_tuning = model_evaluation(classifiers, X_train_sample, y_train_sample, X_val_sample, y_val_sample, kf, params= params_classifiers, mode='tuning')\n",
    "\n",
    "# Display the scores\n",
    "pd.DataFrame(list(zip(model_list_tuning, MCC_train_list_tuning, MCC_val_list_tuning)), columns=['Model Name', 'MCC_Score_Train_sample', 'MCC_Score_val_sample']).sort_values(by=[\"MCC_Score_val_sample\"],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a subset of models that has MCC_Score_val > 0.9\n",
    "models_selected = []\n",
    "parameters_selected =[]\n",
    "for i in range(len(list(classifiers))):\n",
    "    if MCC_val_list_tuning[i] > 0.8:\n",
    "        print(list(classifiers.keys())[i])\n",
    "        models_selected.append(list(classifiers.keys())[i])\n",
    "        parameters_selected.append(model_params_tuning[i])\n",
    "\n",
    "# Display the selected models\n",
    "models_selected\n",
    "\n",
    "# Select the models from the classifiers dictionary\n",
    "classifiers_selected = {key: classifiers[key] for key in models_selected}\n",
    "\n",
    "# add the parameters to the selected models\n",
    "params_classifiers_selected = {key: parameters_selected[i] for i,key in enumerate(models_selected)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_classifiers_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the entire data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train the selected models with the entire training data\n",
    "model_list, MCC_train_list, MCC_val_list, y_train_pred_list, oof_predictions_df, val_predictions_df = model_evaluation(classifiers_selected , X_train_sample, y_train_sample, X_val_sample, y_val_sample, kf, params= params_classifiers_selected, mode='training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
